{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VuCBty_omqJW"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "76427c9fd27b426f814f272a28ea676a": {
          "model_module": "ipycanvas",
          "model_name": "CanvasManagerModel",
          "model_module_version": "^0.13",
          "state": {
            "_model_module": "ipycanvas",
            "_model_module_version": "^0.13",
            "_model_name": "CanvasManagerModel",
            "_view_count": null,
            "_view_module": null,
            "_view_module_version": "",
            "_view_name": null
          }
        },
        "45c02ab56a0b4c33bda6d5020459b478": {
          "model_module": "ipycanvas",
          "model_name": "CanvasModel",
          "model_module_version": "^0.13",
          "state": {
            "_canvas_manager": "IPY_MODEL_76427c9fd27b426f814f272a28ea676a",
            "_dom_classes": [],
            "_model_module": "ipycanvas",
            "_model_module_version": "^0.13",
            "_model_name": "CanvasModel",
            "_send_client_ready_event": true,
            "_view_count": null,
            "_view_module": "ipycanvas",
            "_view_module_version": "^0.13",
            "_view_name": "CanvasView",
            "height": 512,
            "image_data": null,
            "layout": "IPY_MODEL_32d6eac0258044c9ab6ea6f4370f594a",
            "sync_image_data": true,
            "width": 512
          }
        },
        "32d6eac0258044c9ab6ea6f4370f594a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "metadata": {
        "id": "1ws5cFH4VcHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize the MNIST dataset using precomputed mean and standard deviation.\n",
        "# These values help improve training stability and convergence speed by ensuring that the pixel values\n",
        "# are centered around 0 and have a standard deviation of 1.\n",
        "\n",
        "# Precomputed values for the MNIST dataset:\n",
        "#   - Mean (μ) = 0.1307\n",
        "#   - Standard Deviation (σ) = 0.3081"
      ],
      "metadata": {
        "id": "VuCBty_omqJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n"
      ],
      "metadata": {
        "id": "yYigvl1sVep6"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):               # B -> Batch Size\n",
        "        x = self.relu(self.conv1(x))    # find simple patterns -> (B, 32, 28, 28)\n",
        "        x = self.relu(self.conv2(x))    # build more complex patterns -> (B, 64, 28, 28)\n",
        "        x = self.pool(x)                # down‑sample, keep strongest signals  -> (B, 64, 14, 14)\n",
        "        x = x.view(x.size(0), -1)       # flatten to a vector -> (B, 12544)\n",
        "        x = self.relu(self.fc1(x))      # mix into 128 features -> (B, 128)\n",
        "        x = self.fc2(x)                 # final 10 class scores -> (B, 10)\n",
        "        return x"
      ],
      "metadata": {
        "id": "tbi4c5C8VlOQ"
      },
      "execution_count": 317,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(model, train_loader, test_loader, device, epochs=5):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=0.5)  # reduce LR by a factor of 0.5 every epochs, allows big steps then finer tuning\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1} - Training Loss: {running_loss/len(train_loader):.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    torch.save(model.state_dict(), \"model.pth\")\n"
      ],
      "metadata": {
        "id": "jqTKIUsIfBKh"
      },
      "execution_count": 310,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CNNModel().to(device)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "train(model, train_loader, test_loader, device, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL0JZNfefQOY",
        "outputId": "0770a619-21b0-4241-bd5b-fad3c8f971b8"
      },
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 0.1962\n",
            "Epoch 1, Batch 200, Loss: 0.0523\n",
            "Epoch 1, Batch 300, Loss: 0.0316\n",
            "Epoch 1, Batch 400, Loss: 0.0727\n",
            "Epoch 1 - Training Loss: 0.1342, Accuracy: 95.84%\n",
            "Epoch 2, Batch 100, Loss: 0.0419\n",
            "Epoch 2, Batch 200, Loss: 0.0960\n",
            "Epoch 2, Batch 300, Loss: 0.0051\n",
            "Epoch 2, Batch 400, Loss: 0.0114\n",
            "Epoch 2 - Training Loss: 0.0302, Accuracy: 99.09%\n",
            "Epoch 3, Batch 100, Loss: 0.0371\n",
            "Epoch 3, Batch 200, Loss: 0.0112\n",
            "Epoch 3, Batch 300, Loss: 0.0018\n",
            "Epoch 3, Batch 400, Loss: 0.0082\n",
            "Epoch 3 - Training Loss: 0.0162, Accuracy: 99.54%\n",
            "Epoch 4, Batch 100, Loss: 0.0020\n",
            "Epoch 4, Batch 200, Loss: 0.0042\n",
            "Epoch 4, Batch 300, Loss: 0.0065\n",
            "Epoch 4, Batch 400, Loss: 0.0004\n",
            "Epoch 4 - Training Loss: 0.0093, Accuracy: 99.77%\n",
            "Epoch 5, Batch 100, Loss: 0.0136\n",
            "Epoch 5, Batch 200, Loss: 0.0066\n",
            "Epoch 5, Batch 300, Loss: 0.0013\n",
            "Epoch 5, Batch 400, Loss: 0.0029\n",
            "Epoch 5 - Training Loss: 0.0066, Accuracy: 99.87%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Evaluation"
      ],
      "metadata": {
        "id": "uWXziwR40poy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)#fwd\n",
        "\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "i7LMVn0rVmOX"
      },
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "evaluate(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6TAyeAneoB7",
        "outputId": "e7af9117-facd-481e-927e-39cc69fe5c4c"
      },
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 99.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Demo Testing"
      ],
      "metadata": {
        "id": "YQDrSSZe0k55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipywidgets\n",
        "!pip install ipycanvas\n"
      ],
      "metadata": {
        "id": "kEn-GLUGWVJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from ipycanvas import Canvas\n",
        "from PIL import Image\n",
        "from spikingjelly.activation_based import functional\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import time"
      ],
      "metadata": {
        "id": "W8PnIxExVqih"
      },
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start_drawing(x, y):\n",
        "    global drawing\n",
        "    drawing = True\n",
        "    canvas.stroke_style = 'white'\n",
        "    canvas.line_width = 20\n",
        "    canvas.begin_path()\n",
        "    canvas.move_to(x, y)\n",
        "\n",
        "def draw(x, y):\n",
        "    if drawing:\n",
        "        canvas.line_to(x, y)\n",
        "        canvas.stroke()\n",
        "\n",
        "def stop_drawing(x, y):\n",
        "    global drawing\n",
        "    drawing = False\n",
        "    predict_drawing()\n"
      ],
      "metadata": {
        "id": "4-4_zp0RvNo_"
      },
      "execution_count": 506,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_drawing(image):\n",
        "    img = np.array(image)\n",
        "    # to fit input params, image is 512x512, need to put into 28x28\n",
        "    img = Image.fromarray((img).astype(np.uint8)).resize((28, 28))\n",
        "    img = np.array(img)\n",
        "    img = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, 28, 28]\n",
        "    return img\n",
        "\n",
        "def load_model(model_path):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = CNNModel().to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    return model, device\n",
        "\n",
        "def predict_drawing():\n",
        "    model_path = 'model.pth'\n",
        "    model, device = load_model(model_path)\n",
        "\n",
        "    img_data = canvas.get_image_data(0, 0, 512, 512)\n",
        "    img_pil = Image.fromarray(img_data).convert('L')\n",
        "    img_np = np.array(img_pil)\n",
        "\n",
        "    img = process_drawing(img_np)  #  [1, 1, 28, 28] normalized\n",
        "    img = img.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(img)\n",
        "        probabilities = torch.softmax(output, dim=1)\n",
        "        predicted_digit = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "    print(\"Probabilities:\", probabilities.cpu().numpy())\n",
        "    print(f\"Predicted digit: {predicted_digit}\")\n",
        "\n",
        "    img_display = img.squeeze().cpu().numpy()\n",
        "    plt.imshow(img_display, cmap='gray')\n",
        "    plt.title(f\"Predicted: {predicted_digit}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "fAul7dzkGJv7"
      },
      "execution_count": 487,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "canvas = Canvas(width=512, height=512, sync_image_data=True)\n",
        "canvas.fill_style = 'black'\n",
        "canvas.fill_rect(0, 0, canvas.width, canvas.height)\n",
        "\n",
        "# display the canvas in Colab\n",
        "display(canvas)\n",
        "canvas.on_mouse_move(draw)\n",
        "canvas.on_mouse_up(stop_drawing)\n",
        "canvas.on_mouse_down(start_drawing)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 980,
          "referenced_widgets": [
            "45c02ab56a0b4c33bda6d5020459b478",
            "76427c9fd27b426f814f272a28ea676a",
            "32d6eac0258044c9ab6ea6f4370f594a"
          ]
        },
        "id": "Cp454CwZW_Rd",
        "outputId": "babfc00d-3f99-43d2-b716-1c9014b896e4"
      },
      "execution_count": 517,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Canvas(height=512, sync_image_data=True, width=512)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45c02ab56a0b4c33bda6d5020459b478"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "Predicted digit: 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "6iIE_plbWypQ"
      }
    }
  ]
}